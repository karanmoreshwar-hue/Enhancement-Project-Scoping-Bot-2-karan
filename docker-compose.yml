version: '3.8'

services:
  # PostgreSQL Database
  db:
    image: postgres:15-alpine
    container_name: scopebot-db
    restart: always
    environment:
      POSTGRES_USER: scopebot
      POSTGRES_PASSWORD: karan
      POSTGRES_DB: scopebot_db
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    networks:
      - scopebot-network
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U scopebot -d scopebot_db" ]
      interval: 5s
      timeout: 5s
      retries: 5

  # Qdrant Vector Database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: scopebot-qdrant
    restart: always
    ports:
      - "6333:6333"
    volumes:
      - qdrant_storage:/qdrant/storage
    networks:
      - scopebot-network

  # Backend (FastAPI)
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: scopebot-backend
    restart: always
    depends_on:
      db:
        condition: service_healthy
      qdrant:
        condition: service_started
    ports:
      - "8000:8000"
    env_file:
      - ./backend/.env
    environment:
      # Override DB Host to point to docker container
      DATABASE_URL: postgresql+asyncpg://scopebot:karan@db:5432/scopebot_db
      # Override Qdrant Host
      QDRANT_HOST: qdrant
      # Point Ollama to the HOST machine (since it runs locally outside docker)
      # On Linux, host.docker.internal requires extra config or --add-host (added below)
      OLLAMA_HOST: http://host.docker.internal:11434
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - scopebot-network
    volumes:
      - ./backend/uploads:/app/uploads # Persist uploads if any local storage used

  # Frontend (React + Nginx)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: scopebot-frontend
    restart: always
    depends_on:
      - backend
    ports:
      - "3000:80"
    networks:
      - scopebot-network

  # Presenton AI Presentation Generator
  presenton:
    image: ghcr.io/presenton/presenton:latest
    container_name: scopebot-presenton
    restart: unless-stopped
    ports:
      - "5000:80"
    volumes:
      - presenton_data:/app_data
      - ./docker/presenton/patches/llm_client.py:/app/servers/fastapi/services/llm_client.py
      - ./docker/presenton/patches/image_generation_service.py:/app/servers/fastapi/services/image_generation_service.py
      - ./docker/presenton/patches/presentation_endpoint.py:/app/servers/fastapi/api/v1/ppt/endpoints/presentation.py
      - ./docker/presenton/patches/start.js:/app/start.js
      - ./docker/presenton/patches/next.config.mjs:/app/servers/nextjs/next.config.mjs
      - ./docker/presenton/patches/nginx.conf:/etc/nginx/nginx.conf
    env_file:
      - .env
    environment:
      # CRITICAL: Use LLM=openai (not DEFAULT_LLM_PROVIDER) - this is what Presenton actually reads
      - LLM=openai
      # Configure Azure OpenAI (to avoid Google Gemini quota limits)
      - OPENAI_API_KEY=${AZURE_OPENAI_KEY}
      - OPENAI_API_BASE=${AZURE_OPENAI_ENDPOINT}
      - OPENAI_API_VERSION=${AZURE_OPENAI_VERSION}
      - OPENAI_API_TYPE=azure
      - AZURE_OPENAI_DEPLOYMENT=${AZURE_OPENAI_DEPLOYMENT_NAME}
      # Set default model
      - DEFAULT_LLM_MODEL=${AZURE_OPENAI_DEPLOYMENT_NAME}
      # Disable Google Gemini completely
      - GOOGLE_API_KEY=
      - GEMINI_API_KEY=
      - ENABLE_GEMINI=false
      - ENABLE_GEMINI=false
      - USE_AZURE_OPENAI=true
      - IMAGE_PROVIDER=dall-e-3
      - CAN_CHANGE_KEYS=false
    networks:
      - scopebot-network

networks:
  scopebot-network:
    driver: bridge

volumes:
  pgdata:
  qdrant_storage:
  presenton_data:
